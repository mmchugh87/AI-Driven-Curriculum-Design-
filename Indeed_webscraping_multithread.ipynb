{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8012be7",
   "metadata": {},
   "source": [
    "# Web Scraping - Indeed.com\n",
    "General steps for Web Scraping\n",
    "1. Check whether the website allows web scraping\n",
    "2. Obtain the source code (HTML File) by using the website URL\n",
    "3. Download the website content\n",
    "4. Parse the content using keywords tags for elements of interest\n",
    "5. Extract relevant data/features\n",
    "6. Organize raw data in structured format (e.g., CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5876b0ab",
   "metadata": {},
   "source": [
    "### Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20edefb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096d6e0",
   "metadata": {},
   "source": [
    "### Path to webdriver (Firefox, Chrome) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define firefox driver\n",
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77187fd5",
   "metadata": {},
   "source": [
    "### Define position and location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2685a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enter a job position\n",
    "position = \"python analyst\"\n",
    "## Enter a location (City, State or Zip or remote)\n",
    "locations = \"remote\"\n",
    "\n",
    "def get_url(position, location):\n",
    "    url_template = \"https://www.indeed.com/jobs?q={}&l={}\"\n",
    "    url = url_template.format(position, location)\n",
    "    return url\n",
    "\n",
    "url = get_url(position, locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308931c4",
   "metadata": {},
   "source": [
    "### Scrape job postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115efb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of postings to scrape (testing with a low number - can increase to 1000 from testing)\n",
    "postings = 800\n",
    "\n",
    "dataframe = []\n",
    "dataframe = pd.DataFrame(columns=[\"Title\", \"Company\", \"Location\", \"Rating\", \"Date\", \"Salary\", \"Description\", \"Links\"])\n",
    "jn=0\n",
    "for i in range(0, postings, 10):\n",
    "    driver.get(url + \"&start=\" + str(i))\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    jobs = driver.find_elements(By.CLASS_NAME, 'job_seen_beacon')\n",
    "\n",
    "    for job in jobs:\n",
    "        result_html = job.get_attribute('innerHTML')\n",
    "        soup = BeautifulSoup(result_html, 'html.parser')\n",
    "        \n",
    "        jn += 1\n",
    "        \n",
    "        liens = job.find_elements(By.TAG_NAME, \"a\")\n",
    "        links = liens[0].get_attribute(\"href\")\n",
    "        \n",
    "        title = soup.select('.jobTitle')[0].get_text().strip()\n",
    "        try:\n",
    "            company = soup.find_all(attrs={'data-testid': 'company-name'})[0].get_text().strip() \n",
    "\n",
    "        except:\n",
    "            company = 'NaN'\n",
    "            \n",
    "        try:\n",
    "            location = soup.find_all(attrs={'data-testid': 'text-location'})[0].get_text().strip()\n",
    "\n",
    "        except:\n",
    "            location = 'NaN'\n",
    "            \n",
    "        try:\n",
    "            salary = soup.select('.salary-snippet-container')[0].get_text().strip()\n",
    "        except:\n",
    "            salary = 'NaN'\n",
    "            \n",
    "        try:\n",
    "            rating = soup.find(\"div\",{\"class\":\"companyInfo\"}).find(\"span\",{\"class\":\"ratingsDisplay\"}).text #scrapfly version\n",
    "        except:\n",
    "            rating = 'NaN'\n",
    "            \n",
    "        try:\n",
    "            date = soup.find_all('span',attrs={'data-testid': 'myJobsStateDate'})[0].get_text().strip()\n",
    "            words_posted_today = [\"Today\" , \"Just\", \"ongoing\"]\n",
    "            if \"ago\" in date:\n",
    "                date_temp = date.split()\n",
    "                date_temp = date_temp[-3:]\n",
    "                date = (date_temp[0] + ' ' + date_temp[1] + ' ' + date_temp[2])\n",
    "            elif any(x in date for x in words_posted_today):\n",
    "                date = \"0 days ago\"\n",
    "            else:\n",
    "                date = 'NaN'\n",
    "        except:\n",
    "            date = 'NaN'\n",
    "            \n",
    "        try:\n",
    "            description = soup.select('.job-snippet')[0].get_text().strip()\n",
    "        except:\n",
    "            description = ''\n",
    "       \n",
    "        dataframe = pd.concat([dataframe, pd.DataFrame([{'Title': title,\n",
    "                                          \"Company\": company,\n",
    "                                          'Location': location,\n",
    "                                          'Rating': rating,\n",
    "                                          'Date': date,\n",
    "                                          \"Salary\": salary,\n",
    "                                          \"Description\": description,\n",
    "                                          \"Links\": links}])], ignore_index=True)\n",
    "        print(\"Job number {0:4d} added - {1:s}\".format(jn,title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d8ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007e1a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f3eb6e",
   "metadata": {},
   "source": [
    "### Scrape full job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdb15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_dataframe(dataframe, num_parts=8):\n",
    "    total_rows = len(dataframe)\n",
    "    part_size = total_rows // num_parts\n",
    "    dataframe_slices = []\n",
    "\n",
    "    for i in range(0, total_rows, part_size):\n",
    "        start_index = i\n",
    "        end_index = min(i + part_size, total_rows)\n",
    "        dataframe_slices.append(dataframe.iloc[start_index:end_index])\n",
    "\n",
    "    return dataframe_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_slices = split_dataframe(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e6fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_descriptions(dataframe):\n",
    "    driver = webdriver.Firefox()\n",
    "    Links_list = dataframe['Links'].tolist()\n",
    "    descriptions = []\n",
    "    indices_to_remove = []\n",
    "\n",
    "    for index, link in enumerate(Links_list):\n",
    "        driver.get(link)\n",
    "        driver.implicitly_wait(random.randint(3, 8))\n",
    "        \n",
    "        try:\n",
    "            jd = driver.find_element(By.XPATH, '//div[@id=\"jobDescriptionText\"]').text\n",
    "            descriptions.append(jd)\n",
    "        except NoSuchElementException:\n",
    "            print(f\"No job description found for link at index {index}: {link}. Skipping...\")\n",
    "            indices_to_remove.append(index)\n",
    "            continue\n",
    "        \n",
    "        time.sleep(random.randint(5,10))\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    \n",
    "    # Create a boolean mask to filter out rows with indices to remove\n",
    "    mask = ~dataframe.index.isin(indices_to_remove)\n",
    "    \n",
    "    # Filter out rows to keep\n",
    "    dataframe = dataframe[mask].copy()\n",
    "    \n",
    "    # Ensure the lengths of descriptions match the length of the dataframe\n",
    "    if len(descriptions) != len(dataframe):\n",
    "        if len(descriptions) < len(dataframe):\n",
    "            # Pad descriptions with empty strings\n",
    "            descriptions += [''] * (len(dataframe) - len(descriptions))\n",
    "        else:\n",
    "            # Truncate descriptions\n",
    "            descriptions = descriptions[:len(dataframe)]\n",
    "    \n",
    "    # Assign descriptions to dataframe\n",
    "    dataframe['Description'] = descriptions\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9bebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419130a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of threads (workers)\n",
    "num_threads = 4  # Adjust as needed\n",
    "\n",
    "# Create a ThreadPoolExecutor with the specified number of threads\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    # Submit each dataframe slice processing function to the executor\n",
    "    futures = []\n",
    "    for df_slice in dataframe_slices:\n",
    "        futures.append(executor.submit(scrape_descriptions, df_slice))\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        result_df_slice = future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a307dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframes = [futures[0].result(), \n",
    " futures[1].result(), \n",
    " futures[2].result(), \n",
    " futures[3].result(), \n",
    " futures[4].result(), \n",
    " futures[5].result(), \n",
    " futures[6].result(), \n",
    " futures[7].result() \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa781b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat(result_dataframes , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6ac4a",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ed5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe to a csv file\n",
    "#date = datetime.today().strftime('%Y-%m-%d')\n",
    "date = datetime.today().strftime('%Y-%m-%d_%H-%M')\n",
    "concatenated_df.to_csv(date + \"_\" + position + \"_\" + locations + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
